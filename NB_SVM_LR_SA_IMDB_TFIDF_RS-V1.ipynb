{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB_SVM_LR_SA_IMDB_TFIDF_RS\n",
    "\n",
    "## Model 1:\n",
    "1. UseFusing Naive Bayes and SVM/LR\n",
    "2. Do Sentiment Analysis (SA)\n",
    "3. TF-IDF\n",
    "4. Refined Stopwords (RS)\n",
    "\n",
    "The notebook will have the following structure:\n",
    "1. Importing libraries and data\n",
    "2. Exploratory data analysis\n",
    "3. Tokenization and training/validation split\n",
    "4. Naive Bayes with TF-IDF\n",
    "5. Logistic Regression/SVC with NB features\n",
    "7. Inference and submission\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing libraries and data\n",
    "#from fastai.nlp import *\n",
    "import fastai\n",
    "#fastai.download('nlp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "#from fastai.nlp import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from pathlib import Path\n",
    "import os, re, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'train']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path('C:/Users/xbabu/Babu/pers/PhD/MIST.7060 Data Analytics/Project/data/IMDB')\n",
    "names = ['neg', 'pos']\n",
    "# path = Path('../data/Toxic')\n",
    "names = ['neg', 'pos']\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.listdir(path/'train/pos')[0:20]\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def texts_from_files(src, names):\n",
    "    texts,labels = [],[]\n",
    "    for idx,name in enumerate(names):\n",
    "        path = os.path.join(src, name)\n",
    "        t = [o.strip() for o in open(path, encoding = \"ISO-8859-1\")]\n",
    "        texts += t\n",
    "        labels += ([idx] * len(t))\n",
    "    return texts,np.array(labels)\n",
    "\n",
    "def texts_from_folders(src, names):\n",
    "    texts,labels = [],[]\n",
    "    MAX_DOCS = 10000\n",
    "    for idx,name in enumerate(names):\n",
    "        path = os.path.join(src, name)\n",
    "        i = 1\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            fpath = os.path.join(path, fname)\n",
    "            texts.append(open(fpath, encoding=\"utf8\").read())\n",
    "            labels.append(idx)\n",
    "            i = i + 1\n",
    "            if i > MAX_DOCS: \n",
    "                break\n",
    "    return texts,np.array(labels)\n",
    "\n",
    "#def tokenize(s): return re_tok.sub(r' \\1 ', s).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "train, train_y = texts_from_folders(path/'train',names)\n",
    "val, val_y = texts_from_folders(path/'test',names)\n",
    "\n",
    "#??load_files\n",
    "#files = load_files('C:/Users/xbabu/Babu/pers/PhD/MIST.7060 Data Analytics/Project/data/IMDB/train', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemma_tokenizer(corpus): # a method to lemmatize corpus\n",
    "    corpus = ''.join([ch for ch in corpus if ch not in string.punctuation]) # remove punctuation\n",
    "    tokens = nltk.word_tokenize(corpus)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.remove('but')\n",
    "stopwords.remove('no')\n",
    "stopwords.remove('not')\n",
    "#print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Using trigram features actually is going to turn out to make\n",
    "# NB and logistic regression quite a lot better\n",
    "# max_features=1000000, the CountVectorizer will sort the vocabulary by \n",
    "# how often everything appears whether it be unigram/bigram/trigram \n",
    "# and it will cut it off after the defined count (1000000)\n",
    "# most common ngrams\n",
    "#veczr = CountVectorizer(ngram_range=(1,2), tokenizer=tokenize, max_features=800000)\n",
    "#veczr = CountVectorizer(lowercase=False, stop_words='english', tokenizer=re_tokenizer, ngram_range=(1,2), min_df=2, max_df=0.95, max_features=1000000)\n",
    "#veczr = CountVectorizer(lowercase=False, tokenizer=lemma_tokenizer, stop_words=stopwords, ngram_range=(1,2), min_df=2, max_df=0.8, max_features=2000000)\n",
    "vec_tfidf = TfidfVectorizer(ngram_range=(1,4), tokenizer=lemma_tokenizer,  stop_words=stopwords, lowercase=True,\n",
    "               min_df=4, max_df=0.8, strip_accents='unicode', sublinear_tf=True)\n",
    "#trn_term_doc = veczr.fit_transform(train)\n",
    "#val_term_doc = veczr.transform(val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_term_doc = vec_tfidf.fit_transform(train)\n",
    "val_term_doc = vec_tfidf.transform(val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<20000x121974 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3018009 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(trn_term_doc)\n",
    "#trn_term_doc_dense = trn_term_doc.toarray()\n",
    "trn_term_doc\n",
    "#trn_term_doc_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = vec_tfidf.get_feature_names()\n",
    "#dictionary\n",
    "#pd.DataFrame(trn_term_doc_dense, columns=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w0 = set([o.lower() for o in train[1].split(' ')])\n",
    "#w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 20000\n",
      "Test set size: 20000\n"
     ]
    }
   ],
   "source": [
    "#print(len(w0))\n",
    "print(f'Training set size: {len(train)}')\n",
    "print(f'Test set size: {len(val)}')\n",
    "#print(ctvec.vocabulary_['writers'])\n",
    "#trn_term_doc[1, 934]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Naive Bayes\n",
    "\n",
    "We define the **log-count ratio** $r$ for each word $f$:\n",
    "\n",
    "$r = \\log \\frac{\\text{ratio of feature $f$ in positive documents}}{\\text ratio of feature $f$ in negative documents}}$\n",
    "\n",
    "where ratio of feature $f$ in positive documents is the number of times a positive has a feature divided by the number of positive documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with TFIDF NB matrix:\n",
      "Accuracy: 0.87455\n",
      "Confusion matrix:\n",
      "                  predicted positive  predicted negative\n",
      "actual positive                8510                1490\n",
      "actual negative                1019                8981\n",
      "precision = 0.8930632805121209 , recall = 0.851 , F1-score = 0.8715243996108352\n",
      "Elapsed Time: 0.075 seconds.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "t0 = time()\n",
    "x = trn_term_doc\n",
    "y = train_y\n",
    "\n",
    "# grab the rows where the dependent varaible is 1, \n",
    "# then we sum over the rows to get the total word count \n",
    "# for that feature across all the documents \n",
    "# plus 1 to consider the unreviewed probability\n",
    "p = x[y==1].sum(0)+1\n",
    "# grab the rows where the dependent varaible is 0\n",
    "q = x[y==0].sum(0)+1\n",
    "# it is nicer to take the log to add things together \n",
    "# rather than mulitply them together\n",
    "# and once you like multiply enough of these things together it's \n",
    "# going to get kind of so close to zero that you will probably run out of\n",
    "# floating-point, so we take the log of the ratios and \n",
    "# we then multiply that or  \n",
    "# add that to the ratio of the whole class probabilities\n",
    "r = np.log((p/p.sum())/(q/q.sum()))\n",
    "b = np.log(len(p)/len(q))\n",
    "#p\n",
    "#p.sum()\n",
    "#r\n",
    "# So in order to say for each document multiply the\n",
    "# bayes probabilities by the counts, we can just use matrix multiply\n",
    "# and then to add on the log of the class ratios we can just use + b\n",
    "# so we end up with something that looks a lot like our logistic regression.\n",
    "# but we are not learning anything, we are just calculating it using this\n",
    "# theoretical model\n",
    "pre_preds = val_term_doc @r.T + b\n",
    "# Now we compare that as to whether it is bigger or smaller\n",
    "# smaller than zero, not one anymore because we are now in log space \n",
    "preds = pre_preds.T>0\n",
    "# then we can compare that to the mean and we can say\n",
    "# how much it is accurate\n",
    "# so NB is not nothing and it gave us something. \n",
    "(preds == val_y).mean()\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('Results with BOW NB matrix:\\n' + 'Accuracy:', metrics.accuracy_score(val_y, preds.T))\n",
    "cm = pd.DataFrame(confusion_matrix(val_y, preds.T, labels=[1,0]),\n",
    "                  index=['actual positive', 'actual negative'],\n",
    "                  columns=['predicted positive', 'predicted negative'])\n",
    "print('Confusion matrix:\\n', cm)\n",
    "TP = cm.at['actual positive','predicted positive']\n",
    "FP = cm.at['actual negative','predicted positive']\n",
    "FN = cm.at['actual positive','predicted negative']\n",
    "print('precision =', TP/(TP+FP), ', recall =', TP/(TP+FN), ', F1-score =', 2*TP/(2*TP+FP+FN))\n",
    "\n",
    "print(\"Elapsed Time: %0.3f seconds.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula for Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binarized Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with TFIDF NB.sign() matrix:\n",
      "Accuracy: 0.8719\n",
      "Confusion matrix:\n",
      "                  predicted positive  predicted negative\n",
      "actual positive                8312                1688\n",
      "actual negative                 874                9126\n",
      "precision = 0.9048552144567821 , recall = 0.8312 , F1-score = 0.8664651308245596\n",
      "Elapsed Time: 0.063 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Turns out that the version where we are actually looking at\n",
    "# how often a word appears like \"absurd\" appeared twice. It turns out\n",
    "# at least for this problem and quite often it doesn't matter\n",
    "# whether \"absurd\" appeared twice or once\n",
    "# all that matters is it appeared so what we tend to d here is\n",
    "# take the term document matrix and do doc sign.\n",
    "# doc.sign replaces anything positive as 1 and \n",
    "# anything negative with -1 so it binarizes it\n",
    "# it gives a better result\n",
    "t0 = time()\n",
    "pre_preds = val_term_doc.sign() @r.T + b\n",
    "#preds\n",
    "preds = pre_preds.T>0\n",
    "(preds == val_y).mean()\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('Results with TFIDF NB.sign() matrix:\\n' + 'Accuracy:', metrics.accuracy_score(val_y, preds.T))\n",
    "cm = pd.DataFrame(confusion_matrix(val_y, preds.T, labels=[1,0]),\n",
    "                  index=['actual positive', 'actual negative'],\n",
    "                  columns=['predicted positive', 'predicted negative'])\n",
    "print('Confusion matrix:\\n', cm)\n",
    "TP = cm.at['actual positive','predicted positive']\n",
    "FP = cm.at['actual negative','predicted positive']\n",
    "FN = cm.at['actual positive','predicted negative']\n",
    "print('precision =', TP/(TP+FP), ', recall =', TP/(TP+FN), ', F1-score =', 2*TP/(2*TP+FP+FN))\n",
    "print(\"Elapsed Time: %0.3f seconds.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Logistic Regression\n",
    "instead of using the coefficients r.T, let us learn them using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with TFIDF LR :\n",
      "Accuracy: 0.86365\n",
      "Confusion matrix:\n",
      "                  predicted positive  predicted negative\n",
      "actual positive                8679                1321\n",
      "actual negative                1406                8594\n",
      "precision = 0.8605850272682202 , recall = 0.8679 , F1-score = 0.8642270351008216\n",
      "Elapsed Time: 0.629 seconds.\n"
     ]
    }
   ],
   "source": [
    "# In theory NB sounds ok, but rather than assuming that\n",
    "# we should use these coefficients r.T why don't we learn them\n",
    "# let su create a logistic regression\n",
    "m = LogisticRegression(C=0.1, dual=False)\n",
    "# let's fit some coefficients that is going to \n",
    "# literally give us with exactly the same functional form we have above\n",
    "# but now rather than using the theoretical r and b, we are going to\n",
    "# calculate based on the logistic regression and that is better\n",
    "# Theoretical model are never going to be accurate \n",
    "# pretty much as a data-driven model unless we are dealing with some\n",
    "# physics\n",
    "# It is better to learn the coefficients and calculate\n",
    "t0 = time()\n",
    "m.fit(x,y)\n",
    "preds = m.predict(val_term_doc)\n",
    "(preds==val_y).mean()\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('Results with TFIDF LR :\\n' + 'Accuracy:', metrics.accuracy_score(val_y, preds))\n",
    "cm = pd.DataFrame(confusion_matrix(val_y, preds, labels=[1,0]),\n",
    "                  index=['actual positive', 'actual negative'],\n",
    "                  columns=['predicted positive', 'predicted negative'])\n",
    "print('Confusion matrix:\\n', cm)\n",
    "TP = cm.at['actual positive','predicted positive']\n",
    "FP = cm.at['actual negative','predicted positive']\n",
    "FN = cm.at['actual positive','predicted negative']\n",
    "print('precision =', TP/(TP+FP), ', recall =', TP/(TP+FN), ', F1-score =', 2*TP/(2*TP+FP+FN))\n",
    "print(\"Elapsed Time: %0.3f seconds.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with TFIDF LR.sign() matrix:\n",
      "Accuracy: 0.89055\n",
      "Confusion matrix:\n",
      "                  predicted positive  predicted negative\n",
      "actual positive                8918                1082\n",
      "actual negative                1107                8893\n",
      "precision = 0.889576059850374 , recall = 0.8918 , F1-score = 0.8906866416978777\n",
      "Elapsed Time: 1.567 seconds.\n"
     ]
    }
   ],
   "source": [
    "# the term document matrix is much wider than it is tall,\n",
    "# there is a reformulation mathematically, equivalent reformulations of\n",
    "# logistic regression that happens to be a lot faster when it is wider\n",
    "# than it is tall\n",
    "t0 = time()\n",
    "m = LogisticRegression(C=0.1, dual=False)\n",
    "m.fit(trn_term_doc.sign(),y)\n",
    "preds = m.predict(val_term_doc.sign())\n",
    "(preds==val_y).mean()\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('Results with TFIDF LR.sign() matrix:\\n' + 'Accuracy:', metrics.accuracy_score(val_y, preds))\n",
    "cm = pd.DataFrame(confusion_matrix(val_y, preds, labels=[1,0]),\n",
    "                  index=['actual positive', 'actual negative'],\n",
    "                  columns=['predicted positive', 'predicted negative'])\n",
    "print('Confusion matrix:\\n', cm)\n",
    "TP = cm.at['actual positive','predicted positive']\n",
    "FP = cm.at['actual negative','predicted positive']\n",
    "FN = cm.at['actual positive','predicted negative']\n",
    "print('precision =', TP/(TP+FP), ', recall =', TP/(TP+FN), ', F1-score =', 2*TP/(2*TP+FP+FN))\n",
    "print(\"Elapsed Time: %0.3f seconds.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the prediction is going to be a coefficient for every term where it is about 75000 terms in their vocabulary and that seems like a lot of coefficients given that we only got 25,000 reviews so we should try regularizing this. Smaller value to the parameter is more regularizing.\n",
    "Turn on the regularized version by setting C = 0.1\n",
    "avoid overfitting with L2 regularization with binarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m = LogisticRegression(C=0.1, dual=False)\n",
    "#m.fit(trn_term_doc.sign(),y)\n",
    "#preds = m.predict(val_term_doc.sign())\n",
    "#(preds==val_y).mean()\n",
    "\n",
    "# you wouldn't think like 75000 parameters for 25,000 documents it is likely\n",
    "# to overfit indeed it did over fit so it (C=0.1) is adding\n",
    "# L2 regularization to avoid overfitting\n",
    "# L2 is weight square (l2 makes things zero but if two things are highly\n",
    "# correlated then L2 regularization will make move them both\n",
    "# down together, it won't make one of them zero and one of them non-zero) and \n",
    "# L1 is absolute value of weight; L1 has a regularization actually has the \n",
    "# property that it will try to make as many things zero as possible\n",
    "# where as L2 regularization has a property that it tends to make\n",
    "# kind of everything smaller.\n",
    "# But we don't consider that difference in modern ML and we try to \n",
    "# directly interpret the coefficients and we try to understand the models\n",
    "# through interrogation using the kind of techniques \n",
    "# The reason why we care about L1 vs L2 is simply like which one\n",
    "# ends up with a better error on the vali dation set\n",
    "# In this case, the L2 turns out to be a lot faster because you can't use\n",
    "# (dual=True) unless you have L2 and L2 is a default\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with TFIDF matrix:\n",
      "Accuracy: 0.8962\n",
      "Results with TFIDF SVM matrix:\n",
      "Accuracy: 0.8962\n",
      "Confusion matrix:\n",
      "                  predicted positive  predicted negative\n",
      "actual positive                8878                1122\n",
      "actual negative                 954                9046\n",
      "precision = 0.9029698942229455 , recall = 0.8878 , F1-score = 0.8953206938281565\n",
      "Elapsed Time: 994.287 seconds.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "t0 = time()\n",
    "svc = SVC(kernel='linear', C=1)\n",
    "svc.fit(x,y)\n",
    "#print(\"Elapsed Time: %0.3f seconds.\" % (time() - t0))\n",
    "#t0 = time()\n",
    "preds = svc.predict(val_term_doc)\n",
    "print('Results with TFIDF matrix:\\n' + 'Accuracy:', metrics.accuracy_score(val_y, preds))\n",
    "#print(\"Elapsed Time: %0.3f seconds.\" % (time() - t0))\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('Results with TFIDF SVM matrix:\\n' + 'Accuracy:', metrics.accuracy_score(val_y, preds))\n",
    "cm = pd.DataFrame(confusion_matrix(val_y, preds, labels=[1,0]),\n",
    "                  index=['actual positive', 'actual negative'],\n",
    "                  columns=['predicted positive', 'predicted negative'])\n",
    "print('Confusion matrix:\\n', cm)\n",
    "TP = cm.at['actual positive','predicted positive']\n",
    "FP = cm.at['actual negative','predicted positive']\n",
    "FN = cm.at['actual positive','predicted negative']\n",
    "print('precision =', TP/(TP+FP), ', recall =', TP/(TP+FN), ', F1-score =', 2*TP/(2*TP+FP+FN))\n",
    "print(\"Elapsed Time: %0.3f seconds.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with TFIDF matrix:\n",
      "Accuracy: 0.8785\n",
      "Results with TFIDF SVM.sign() matrix:\n",
      "Accuracy: 0.8785\n",
      "Confusion matrix:\n",
      "                  predicted positive  predicted negative\n",
      "actual positive                8747                1253\n",
      "actual negative                1177                8823\n",
      "precision = 0.8813986295848448 , recall = 0.8747 , F1-score = 0.878036538847621\n",
      "Elapsed Time: 1400.764 seconds.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "t0 = time()\n",
    "svc = SVC(kernel='linear', C=1)\n",
    "svc.fit(trn_term_doc.sign(),y)\n",
    "#print(\"Elapsed Time: %0.3f seconds.\" % (time() - t0))\n",
    "#t0 = time()\n",
    "preds = svc.predict(val_term_doc.sign())\n",
    "print('Results with TFIDF matrix:\\n' + 'Accuracy:', metrics.accuracy_score(val_y, preds))\n",
    "#print(\"Elapsed Time: %0.3f seconds.\" % (time() - t0))\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('Results with TFIDF SVM.sign() matrix:\\n' + 'Accuracy:', metrics.accuracy_score(val_y, preds))\n",
    "cm = pd.DataFrame(confusion_matrix(val_y, preds, labels=[1,0]),\n",
    "                  index=['actual positive', 'actual negative'],\n",
    "                  columns=['predicted positive', 'predicted negative'])\n",
    "print('Confusion matrix:\\n', cm)\n",
    "TP = cm.at['actual positive','predicted positive']\n",
    "FP = cm.at['actual negative','predicted positive']\n",
    "FN = cm.at['actual positive','predicted negative']\n",
    "print('precision =', TP/(TP+FP), ', recall =', TP/(TP+FN), ', F1-score =', 2*TP/(2*TP+FP+FN))\n",
    "print(\"Elapsed Time: %0.3f seconds.\" % (time() - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = trn_term_doc.sign()\n",
    "y = train_y\n",
    "# x[y==1].sum(0) is to calculate the ratio of each feature in positive and no negative by dividing the toekn counts.\n",
    "\n",
    "val_x = val_term_doc.sign()\n",
    "p = x[y==1].sum(0)+1\n",
    "q = x[y==0].sum(0)+1\n",
    "r = np.log((p/p.sum())/(q/q.sum()))\n",
    "b = np.log(len(p)/len(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with TFIDF NB-LR matrix:\n",
      "Accuracy: 0.9035\n",
      "Confusion matrix:\n",
      "                  predicted positive  predicted negative\n",
      "actual positive                9049                 951\n",
      "actual negative                 979                9021\n",
      "precision = 0.9023733546071001 , recall = 0.9049 , F1-score = 0.9036349111244258\n",
      "Elapsed Time: 1.233 seconds.\n"
     ]
    }
   ],
   "source": [
    "################ NB-LR #############################\n",
    "# take the term-document matrix and multiply by r\n",
    "# every where 0 appears, 0 appears in x_nb and \n",
    "# every where 1 appears in TD, the r appears there\n",
    "t0 = time()\n",
    "x_nb = x.multiply(r)\n",
    "# we are going to use x_nb as independent variables instead in the\n",
    "# logistic regression\n",
    "m = LogisticRegression(dual=False, C=0.1)\n",
    "m.fit(x_nb, y)\n",
    "\n",
    "# do the samething for the validation set and get the predictions\n",
    "# to get a better result\n",
    "val_x_nb = val_x.multiply(r)\n",
    "preds = m.predict(val_x_nb)\n",
    "#(preds.T==val_y).mean()\n",
    "print('Results with TFIDF NB-LR matrix:\\n' + 'Accuracy:', metrics.accuracy_score(val_y, preds.T))\n",
    "cm = pd.DataFrame(confusion_matrix(val_y, preds.T, labels=[1,0]),\n",
    "                  index=['actual positive', 'actual negative'],\n",
    "                  columns=['predicted positive', 'predicted negative'])\n",
    "print('Confusion matrix:\\n', cm)\n",
    "TP = cm.at['actual positive','predicted positive']\n",
    "FP = cm.at['actual negative','predicted positive']\n",
    "FN = cm.at['actual positive','predicted negative']\n",
    "print('precision =', TP/(TP+FP), ', recall =', TP/(TP+FN), ', F1-score =', 2*TP/(2*TP+FP+FN))\n",
    "print(\"Elapsed Time: %0.3f seconds.\" % (time() - t0))\n",
    "\n",
    "# regularization we start out with our \n",
    "# Loss = Cross entropy loss based on the predictions and the actuals + penalty\n",
    "# L = X.C.(XW, y) + a W^2\n",
    "# If you weights a large then that a.W^2 gets bigger and\n",
    "# it drowns out X.C.(XW, Y) and we want it to be a good fit\n",
    "# so we want to have as little regularization going on as we can get away with\n",
    "# so we want to have lesser weights\n",
    "# the weights that are close to zero were kind of not there.\n",
    "\n",
    "# x_nb is creating kind of variation in weights that is going to cause that\n",
    "# the W^2 to go up \n",
    "# When the input is already mulitplied by r is saying penalize things\n",
    "# where we are varying from our NB prior\n",
    "# I don't like to rely on the theory but if I have some theory then maybe\n",
    "# we can use that as our starting point rather than starting off by assuming\n",
    "# everything's equal, so our prior said we got this model quite Naive Bayes,\n",
    "# and NB model said if NB assumptions were correct then r is the correct\n",
    "# coefficient in this specific formulation because our prior is based\n",
    "# on that theory\n",
    "\n",
    "# that meeans, in a ML technique we can impute them with this kind of \n",
    "# Bayesian sense by starting out by incorporating the theoretical \n",
    "# expectations into the data that we feed to our model. \n",
    "# When we to do so, we don't need to regularize as much.\n",
    "# If we regularize a lot, \n",
    "\n",
    "# C=1e-5 is the reciprocal of the amount of vectorization penalty so \n",
    "# we add lots of regularization by making C value very small\n",
    "# that really hurts the accuracy, because it is really trying hard to get\n",
    "# those weights down, the loss function is overwhelmed by the need to \n",
    "# reduce the weights and the need to make it predictive is \n",
    "# kind of now seen as totally unimportant\n",
    "# So don't push the weights down so that you end up ignoring the terms\n",
    "# but instead push them down so that you try to get rid of ignore \n",
    "# differences from our expectationi based on the NB formulation\n",
    "\n",
    "# In deep learning we can get 94% accuracy,, but still particularly for a\n",
    "# linear technique that's easy, fast and intuitive, this is pretty good and \n",
    "# we used trigrams instead of bigrams \n",
    "\n",
    "# Summary, it is mathematically equivalent to a normal logistic regression\n",
    "# and the differene is actually in the regularization \n",
    "\n",
    "\n",
    "# Regularized Logistic Regresssion\n",
    "# LR is prone to overfitting if you fit it with a very high order polymomial \n",
    "# features, like using a sigmoid function and it may end up with the hypothesis \n",
    "# whose decision boundary is a overly complex and extremely contorted function \n",
    "# that is not really a great hypothesis for a training set\n",
    "# Also, in general, LR with a lot of features, it can end up with overfitting\n",
    "# So we need to change the cost function of LR by adding the term lambda over 2m and sum from j = 1 to n for Theta j square\n",
    "# This has the effect of penalizing the parameters Theta 1, 2 and so on...\n",
    "# Even if we use high order poloymial with a lot of parameters, as long as we apply regularization and keep the parameters small\n",
    "# the decision boundary will be more reasonable for separating out the positive and negative examples.\n",
    "# Tuning parameter -- Lambda helps us explore how much we put emphasis on fitting the data versus making the magnitude of coefficient small\n",
    "# When lambda is 0, we might get the standard likelihood solution i.e. unpenalized MLE solution\n",
    "# When the lambda is infinity, the optimization becomes only care about penalizing the large coefficients of the parameters\n",
    "# That means we are not fitting the data at all because of setting all the parameters to 0.\n",
    "# We need a optimzied lambda which would balance the overfit and the magnitude of the parameter of the coefficients\n",
    "# Finding the optimized lambda is called L2 regularized logistic regression\n",
    "# Bias-Variance tradeoff is controlled by lambda\n",
    "# When lambda is large can lead to large bias since theycan't fit the data very well and we have low variance\n",
    "# When lambda is small, we get very good fit to the training data, so we have low bias, but we can have a very high variance\n",
    "# if the data changed a little bit we get a completely different decision boundary \n",
    "\n",
    "# large can lead to large bias since theycan't fit the data very well and we have low variance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 554.101 seconds.\n",
      "Results with TFIDF NB-SVM matrix:\n",
      "Accuracy: 0.8821\n",
      "Confusion matrix:\n",
      "                  predicted positive  predicted negative\n",
      "actual positive                8738                1262\n",
      "actual negative                1096                8904\n",
      "precision = 0.8885499288183852 , recall = 0.8738 , F1-score = 0.8811132398910961\n",
      "Elapsed Time: 137.037 seconds.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "t0 = time()\n",
    "svc = SVC(kernel='linear', C=1e5)\n",
    "svc.fit(x_nb, y)\n",
    "print(\"Elapsed Time: %0.3f seconds.\" % (time() - t0))\n",
    "\n",
    "t0 = time()\n",
    "preds = svc.predict(val_x_nb)\n",
    "print('Results with TFIDF NB-SVM matrix:\\n' + 'Accuracy:', metrics.accuracy_score(val_y, preds.T))\n",
    "cm = pd.DataFrame(confusion_matrix(val_y, preds, labels=[1,0]),\n",
    "                  index=['actual positive', 'actual negative'],\n",
    "                  columns=['predicted positive', 'predicted negative'])\n",
    "print('Confusion matrix:\\n', cm)\n",
    "TP = cm.at['actual positive','predicted positive']\n",
    "FP = cm.at['actual negative','predicted positive']\n",
    "FN = cm.at['actual positive','predicted negative']\n",
    "print('precision =', TP/(TP+FP), ', recall =', TP/(TP+FN), ', F1-score =', 2*TP/(2*TP+FP+FN))\n",
    "print(\"Elapsed Time: %0.3f seconds.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Trigram with NB Features\n",
    "Logistic regression with Naive Bayes features descrived here. For every document we compute binarized features as described above, but this time we use bigrams and trigrams too. Each feature is a log-count ratio. A logistic regression model is then trained to predict sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemma_tokenizer(corpus): # a method to lemmatize corpus\n",
    "    corpus = ''.join([ch for ch in corpus if ch not in string.punctuation]) # remove punctuation\n",
    "    tokens = nltk.word_tokenize(corpus)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.remove('but')\n",
    "stopwords.remove('no')\n",
    "stopwords.remove('not')\n",
    "#print(stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Using trigram features actually is going to turn out to make\n",
    "# NB and logistic regression quite a lot better\n",
    "# max_features=1000000, the CountVectorizer will sort the vocabulary by \n",
    "# how often everything appears whether it be unigram/bigram/trigram \n",
    "# and it will cut it off after the defined count (1000000)\n",
    "# most common ngrams\n",
    "#veczr = CountVectorizer(ngram_range=(1,2), tokenizer=tokenize, max_features=800000)\n",
    "#veczr = CountVectorizer(lowercase=False, stop_words='english', tokenizer=re_tokenizer, ngram_range=(1,2), min_df=2, max_df=0.95, max_features=1000000)\n",
    "#veczr = CountVectorizer(lowercase=False, tokenizer=lemma_tokenizer, stop_words=stopwords, ngram_range=(1,2), min_df=2, max_df=0.8, max_features=2000000)\n",
    "vec_tfidf = TfidfVectorizer(ngram_range=(1,5), tokenizer=lemma_tokenizer,  stop_words=stopwords, lowercase=True,\n",
    "               min_df=4, max_df=0.8, strip_accents='unicode', sublinear_tf=True)\n",
    "#trn_term_doc = veczr.fit_transform(train)\n",
    "#val_term_doc = veczr.transform(val)\n",
    "trn_term_doc = vec_tfidf.fit_transform(train)\n",
    "val_term_doc = vec_tfidf.transform(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis using Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results:\n",
      "Accuracy: 0.6903\n",
      "Confusion matrix:\n",
      "                  predicted positive  predicted negative\n",
      "actual positive                8517                1483\n",
      "actual negative                4711                5289\n",
      "precision = 0.643861505896583 , recall = 0.8517 , F1-score = 0.7333390735319442\n",
      "Elapsed Time: 159.507 seconds.\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#reviews1 = pd.DataFrame(train, columns=['texts'])[0]\n",
    "from time import time\n",
    "t0 = time()\n",
    "reviews = pd.DataFrame(train, columns=['texts'])\n",
    "sentiments = pd.DataFrame(train_y, columns=['labels'])\n",
    "sentiments = sentiments['labels'].replace(to_replace=[1, 0], value=['positive', 'negative'])\n",
    "\n",
    "def predict_sentiment(review, printout, sentiment):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    #print(review)\n",
    "    scores = analyzer.polarity_scores(review)  # scores has 4 values: neg, neu, pos, compound\n",
    "    #print(scores)\n",
    "    #pred = 'positive' if scores['compound'] > 0 else 'negative' if scores['compound'] < 0 else 'neutral'\n",
    "    pred = 'positive' if scores['compound'] > 0 else 'negative'\n",
    "    if (printout==True):\n",
    "        print(review)\n",
    "        print('neg',scores['neg'],', neu',scores['neu'],', pos',scores['pos'],', normalized_sum',scores['compound'])\n",
    "        print('Predicted:', pred, ', Actual:', sentiment, '\\n')\n",
    "    return pred\n",
    "\n",
    "#reviews = textdf.Text\n",
    "#y = textdf.Sentiment\n",
    "pred_list = [predict_sentiment(review, printout=False, sentiment=sentiments[0]) for review in reviews.texts]\n",
    "predicted = pd.DataFrame(pred_list, columns=['pred'])\n",
    "\n",
    "#print(\"Predicted:\", predicted)\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('Evaluation results:\\n' + 'Accuracy:', metrics.accuracy_score(sentiments, predicted))\n",
    "cm = pd.DataFrame(confusion_matrix(sentiments, predicted, labels=['positive', 'negative']),\n",
    "                  index=['actual positive', 'actual negative'],\n",
    "                  columns=['predicted positive', 'predicted negative'])\n",
    "print('Confusion matrix:\\n', cm)\n",
    "TP = cm.at['actual positive','predicted positive']\n",
    "FP = cm.at['actual negative','predicted positive']\n",
    "FN = cm.at['actual positive','predicted negative']\n",
    "print('precision =', TP/(TP+FP), ', recall =', TP/(TP+FN), ', F1-score =', 2*TP/(2*TP+FP+FN))\n",
    "print(\"Elapsed Time: %0.3f seconds.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20000, 120819), (20000, 120819))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_term_doc.shape, val_term_doc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 120819\n"
     ]
    }
   ],
   "source": [
    "vocab = vec_tfidf.get_feature_names()\n",
    "print(f'Vocabulary size: {len(vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adverse', 'adversity', 'advert', 'advertise', 'advertised']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[3100:3105]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **log-count ratio** $r$ for feature $f$ is the following:\n",
    "\n",
    "f(a) = \\frac{1}{2\\pi i} \\oint\\frac{f(z)}{z-a}dz\n",
    "\n",
    "$r = log \\\\frac{{ratio of feature $f$ in disaster tweets}{text{ratio of feature $f$ in no disaster tweets}}$\n",
    "\n",
    "\n",
    "where features $f$ in the case of NLP are the tokens in our vocabulary (unigrams and bigrams). We will slowly build up to that equation in the next lines of code in order to understand better what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = trn_term_doc.sign()\n",
    "y = train_y\n",
    "# x[y==1].sum(0) is to calculate the ratio of each feature in positive and no negative by dividing the toekn counts.\n",
    "\n",
    "val_x = val_term_doc.sign()\n",
    "p = x[y==1].sum(0)+1\n",
    "q = x[y==0].sum(0)+1\n",
    "r = np.log((p/p.sum())/(q/q.sum()))\n",
    "b = np.log(len(p)/len(q))\n",
    "#r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pre_preds = val_x @r.T + b\n",
    "#preds\n",
    "preds = pre_preds.T>0\n",
    "(preds == val_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularized Logistic Regression where the features are trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with TFIDF matrix:\n",
      "Accuracy: 0.89065\n",
      "Confusion matrix:\n",
      "                  predicted positive  predicted negative\n",
      "actual positive                8920                1080\n",
      "actual negative                1107                8893\n",
      "precision = 0.8895980851700409 , recall = 0.892 , F1-score = 0.8907974234783043\n"
     ]
    }
   ],
   "source": [
    "m = LogisticRegression(C=0.1, dual=False)\n",
    "m.fit(x,y)\n",
    "preds = m.predict(val_x)\n",
    "(preds==val_y).mean()\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('Results with TFIDF matrix:\\n' + 'Accuracy:', metrics.accuracy_score(val_y, preds))\n",
    "cm = pd.DataFrame(confusion_matrix(val_y, preds, labels=[1,0]),\n",
    "                  index=['actual positive', 'actual negative'],\n",
    "                  columns=['predicted positive', 'predicted negative'])\n",
    "print('Confusion matrix:\\n', cm)\n",
    "TP = cm.at['actual positive','predicted positive']\n",
    "FP = cm.at['actual negative','predicted positive']\n",
    "FN = cm.at['actual positive','predicted negative']\n",
    "print('precision =', TP/(TP+FP), ', recall =', TP/(TP+FN), ', F1-score =', 2*TP/(2*TP+FP+FN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the log-count ratio r.\n",
    "\n",
    "Now let us combine NB and Logistic regression together\n",
    "is better than either\n",
    "then use deep learning model to get a pretty much \n",
    "state-of-the-art result for structured learning \n",
    "\n",
    "r is a vector of rank 1 and length equals to number of features.\n",
    "also logistic regression coefficients also rank 1 \n",
    "and length equals to number of features.\n",
    "calculating rank based on theory and based on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative values are -ve feedback and positive values are +ve feedback\n",
    "#r.shape, r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e to the power of that, these are the ones we can compare to 1\n",
    "# rather than to 0\n",
    "\n",
    "#np.exp(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we fit regularized logistic regression where the fatures are the trigram's log-count ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with TFIDF matrix:\n",
      "Accuracy: 0.9034\n",
      "Confusion matrix:\n",
      "                  predicted positive  predicted negative\n",
      "actual positive                9049                 951\n",
      "actual negative                 981                9019\n",
      "precision = 0.9021934197407777 , recall = 0.9049 , F1-score = 0.9035446829755367\n"
     ]
    }
   ],
   "source": [
    "# take the term-document matrix and multiply by r\n",
    "# every where 0 appears, 0 appears in x_nb and \n",
    "# every where 1 appears in TD, the r appears there\n",
    "x_nb = x.multiply(r)\n",
    "# we are going to use x_nb as independent variables instead in the\n",
    "# logistic regression\n",
    "m = LogisticRegression(dual=False, C=0.1)\n",
    "m.fit(x_nb, y)\n",
    "\n",
    "# do the samething for the validation set and get the predictions\n",
    "# to get a better result\n",
    "val_x_nb = val_x.multiply(r)\n",
    "preds = m.predict(val_x_nb)\n",
    "#(preds.T==val_y).mean()\n",
    "print('Results with TFIDF matrix:\\n' + 'Accuracy:', metrics.accuracy_score(val_y, preds.T))\n",
    "cm = pd.DataFrame(confusion_matrix(val_y, preds.T, labels=[1,0]),\n",
    "                  index=['actual positive', 'actual negative'],\n",
    "                  columns=['predicted positive', 'predicted negative'])\n",
    "print('Confusion matrix:\\n', cm)\n",
    "TP = cm.at['actual positive','predicted positive']\n",
    "FP = cm.at['actual negative','predicted positive']\n",
    "FN = cm.at['actual positive','predicted negative']\n",
    "print('precision =', TP/(TP+FP), ', recall =', TP/(TP+FN), ', F1-score =', 2*TP/(2*TP+FP+FN))\n",
    "\n",
    "# regularization we start out with our \n",
    "# Loss = Cross entropy loss based on the predictions and the actuals + penalty\n",
    "# L = X.C.(XW, y) + a W^2\n",
    "# If you weights a large then that a.W^2 gets bigger and\n",
    "# it drowns out X.C.(XW, Y) and we want it to be a good fit\n",
    "# so we want to have as little regularization going on as we can get away with\n",
    "# so we want to have lesser weights\n",
    "# the weights that are close to zero were kind of not there.\n",
    "\n",
    "# x_nb is creating kind of variation in weights that is going to cause that\n",
    "# the W^2 to go up \n",
    "# When the input is already mulitplied by r is saying penalize things\n",
    "# where we are varying from our NB prior\n",
    "# I don't like to rely on the theory but if I have some theory then maybe\n",
    "# we can use that as our starting point rather than starting off by assuming\n",
    "# everything's equal, so our prior said we got this model quite Naive Bayes,\n",
    "# and NB model said if NB assumptions were correct then r is the correct\n",
    "# coefficient in this specific formulation because our prior is based\n",
    "# on that theory\n",
    "\n",
    "# that meeans, in a ML technique we can impute them with this kind of \n",
    "# Bayesian sense by starting out by incorporating the theoretical \n",
    "# expectations into the data that we feed to our model. \n",
    "# When we to do so, we don't need to regularize as much.\n",
    "# If we regularize a lot, \n",
    "\n",
    "# C=1e-5 is the reciprocal of the amount of vectorization penalty so \n",
    "# we add lots of regularization by making C value very small\n",
    "# that really hurts the accuracy, because it is really trying hard to get\n",
    "# those weights down, the loss function is overwhelmed by the need to \n",
    "# reduce the weights and the need to make it predictive is \n",
    "# kind of now seen as totally unimportant\n",
    "# So don't push the weights down so that you end up ignoring the terms\n",
    "# but instead push them down so that you try to get rid of ignore \n",
    "# differences from our expectationi based on the NB formulation\n",
    "\n",
    "# In deep learning we can get 94% accuracy,, but still particularly for a\n",
    "# linear technique that's easy, fast and intuitive, this is pretty good and \n",
    "# we used trigrams instead of bigrams \n",
    "\n",
    "# Summary, it is mathematically equivalent to a normal logistic regression\n",
    "# and the differene is actually in the regularization \n",
    "\n",
    "\n",
    "# Regularized Logistic Regresssion\n",
    "# LR is prone to overfitting if you fit it with a very high order polymomial \n",
    "# features, like using a sigmoid function and it may end up with the hypothesis \n",
    "# whose decision boundary is a overly complex and extremely contorted function \n",
    "# that is not really a great hypothesis for a training set\n",
    "# Also, in general, LR with a lot of features, it can end up with overfitting\n",
    "# So we need to change the cost function of LR by adding the term lambda over 2m and sum from j = 1 to n for Theta j square\n",
    "# This has the effect of penalizing the parameters Theta 1, 2 and so on...\n",
    "# Even if we use high order poloymial with a lot of parameters, as long as we apply regularization and keep the parameters small\n",
    "# the decision boundary will be more reasonable for separating out the positive and negative examples.\n",
    "# Tuning parameter -- Lambda helps us explore how much we put emphasis on fitting the data versus making the magnitude of coefficient small\n",
    "# When lambda is 0, we might get the standard likelihood solution i.e. unpenalized MLE solution\n",
    "# When the lambda is infinity, the optimization becomes only care about penalizing the large coefficients of the parameters\n",
    "# That means we are not fitting the data at all because of setting all the parameters to 0.\n",
    "# We need a optimzied lambda which would balance the overfit and the magnitude of the parameter of the coefficients\n",
    "# Finding the optimized lambda is called L2 regularized logistic regression\n",
    "# Bias-Variance tradeoff is controlled by lambda\n",
    "# When lambda is large can lead to large bias since theycan't fit the data very well and we have low variance\n",
    "# When lambda is small, we get very good fit to the training data, so we have low bias, but we can have a very high variance\n",
    "# if the data changed a little bit we get a completely different decision boundary \n",
    "\n",
    "# large can lead to large bias since theycan't fit the data very well and we have low variance\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with TFIDF matrix:\n",
      "Accuracy: 0.89065\n",
      "Confusion matrix:\n",
      "                  predicted positive  predicted negative\n",
      "actual positive                8920                1080\n",
      "actual negative                1107                8893\n",
      "precision = 0.8895980851700409 , recall = 0.892 , F1-score = 0.8907974234783043\n"
     ]
    }
   ],
   "source": [
    "m = LogisticRegression(C=0.1, dual=False)\n",
    "m.fit(x,y)\n",
    "preds = m.predict(val_x)\n",
    "#print((preds==val_y).mean())\n",
    "print('Results with TFIDF matrix:\\n' + 'Accuracy:', metrics.accuracy_score(val_y, preds))\n",
    "cm = pd.DataFrame(confusion_matrix(val_y, preds, labels=[1,0]),\n",
    "                  index=['actual positive', 'actual negative'],\n",
    "                  columns=['predicted positive', 'predicted negative'])\n",
    "print('Confusion matrix:\\n', cm)\n",
    "TP = cm.at['actual positive','predicted positive']\n",
    "FP = cm.at['actual negative','predicted positive']\n",
    "FN = cm.at['actual positive','predicted negative']\n",
    "print('precision =', TP/(TP+FP), ', recall =', TP/(TP+FN), ', F1-score =', 2*TP/(2*TP+FP+FN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 943.586 seconds.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "t0 = time()\n",
    "svc = SVC(kernel='linear', C=1)\n",
    "svc.fit(x, y)\n",
    "print(\"Elapsed Time: %0.3f seconds.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with TFIDF matrix:\n",
      "Accuracy: 0.87835\n",
      "Elapsed Time: 162.961 seconds.\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "preds = svc.predict(val_x)\n",
    "print('Results with TFIDF matrix:\\n' + 'Accuracy:', metrics.accuracy_score(val_y, preds))\n",
    "print(\"Elapsed Time: %0.3f seconds.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results:\n",
      "Accuracy: 0.87835\n"
     ]
    }
   ],
   "source": [
    "#val_y_df = pval_y\n",
    "#reviews = pd.DataFrame(train, columns=['texts'])\n",
    "#val_y = sentiments['labels'].replace(to_replace=[1, 0], value=['positive', 'negative'])\n",
    "#val_y_df\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('Evaluation results:\\n' + 'Accuracy:', metrics.accuracy_score(val_y, preds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "                  predicted positive  predicted negative\n",
      "actual positive                8744                1256\n",
      "actual negative                1177                8823\n",
      "precision = 0.8813627658502167 , recall = 0.8744 , F1-score = 0.8778675769288691\n"
     ]
    }
   ],
   "source": [
    "cm = pd.DataFrame(confusion_matrix(val_y, preds, labels=[1,0]),\n",
    "                  index=['actual positive', 'actual negative'],\n",
    "                  columns=['predicted positive', 'predicted negative'])\n",
    "print('Confusion matrix:\\n', cm)\n",
    "TP = cm.at['actual positive','predicted positive']\n",
    "FP = cm.at['actual negative','predicted positive']\n",
    "FN = cm.at['actual positive','predicted negative']\n",
    "print('precision =', TP/(TP+FP), ', recall =', TP/(TP+FN), ', F1-score =', 2*TP/(2*TP+FP+FN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 483.631 seconds.\n",
      "Results with TFIDF matrix:\n",
      "Accuracy: 0.8795\n",
      "Elapsed Time: 119.355 seconds.\n",
      "Confusion matrix:\n",
      "                  predicted positive  predicted negative\n",
      "actual positive                8721                1279\n",
      "actual negative                1131                8869\n",
      "precision = 0.8852009744214373 , recall = 0.8721 , F1-score = 0.8786016522264759\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "t0 = time()\n",
    "svc = SVC(kernel='linear', C=1e5)\n",
    "svc.fit(x_nb, y)\n",
    "print(\"Elapsed Time: %0.3f seconds.\" % (time() - t0))\n",
    "\n",
    "t0 = time()\n",
    "preds = svc.predict(val_x_nb)\n",
    "print('Results with TFIDF matrix:\\n' + 'Accuracy:', metrics.accuracy_score(val_y, preds.T))\n",
    "print(\"Elapsed Time: %0.3f seconds.\" % (time() - t0))\n",
    "cm = pd.DataFrame(confusion_matrix(val_y, preds, labels=[1,0]),\n",
    "                  index=['actual positive', 'actual negative'],\n",
    "                  columns=['predicted positive', 'predicted negative'])\n",
    "print('Confusion matrix:\\n', cm)\n",
    "TP = cm.at['actual positive','predicted positive']\n",
    "FP = cm.at['actual negative','predicted positive']\n",
    "FN = cm.at['actual positive','predicted negative']\n",
    "print('precision =', TP/(TP+FP), ', recall =', TP/(TP+FN), ', F1-score =', 2*TP/(2*TP+FP+FN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.1205\n",
      "Root Mean Squared Error: 0.34713109915419565\n",
      "Mean Absolute Error: 0.8795\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import math\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#pred_y = m.predict(x)\n",
    "#(preds==val_y).mean()\n",
    "\n",
    "mse=mean_squared_error(val_y, preds)\n",
    "print(\"Mean Squared Error:\",mse)\n",
    "rmse=math.sqrt(mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "\n",
    "mse=mean_absolute_error(val_y, preds)\n",
    "print(\"Mean Absolute Error:\",1-mse)\n",
    "#preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
