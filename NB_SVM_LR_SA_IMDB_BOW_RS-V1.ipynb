{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB_SVM_LR_SENTIMENTANALYSIS_IMDB_BOW_RS\n",
    "\n",
    "## Model 1:\n",
    "1. UseFusing Naive Bayes and SVM/LR\n",
    "2. Do Sentiment Analysis (SA)\n",
    "3. Use Bag of Words (BOW)\n",
    "4. Refined Stopwords (RS)\n",
    "\n",
    "The notebook will have the following structure:\n",
    "1. Importing libraries and data\n",
    "2. Exploratory data analysis\n",
    "3. Tokenization and training/validation split\n",
    "4. Naive Bayes with bag-of-words\n",
    "5. Naive Bayes with tf-idf\n",
    "6. Logistic Regression/SVC with NB features\n",
    "7. Inference and submission\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "#from fastai.nlp import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from pathlib import Path\n",
    "import os, re, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'train']"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path('C:/Users/xbabu/Babu/pers/PhD/MIST.7060 Data Analytics/Project/data/IMDB')\n",
    "names = ['neg', 'pos']\n",
    "# path = Path('../data/Toxic')\n",
    "names = ['neg', 'pos']\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.listdir(path/'train/pos')[0:20]\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def texts_from_files(src, names):\n",
    "    texts,labels = [],[]\n",
    "    for idx,name in enumerate(names):\n",
    "        path = os.path.join(src, name)\n",
    "        t = [o.strip() for o in open(path, encoding = \"ISO-8859-1\")]\n",
    "        texts += t\n",
    "        labels += ([idx] * len(t))\n",
    "    return texts,np.array(labels)\n",
    "\n",
    "def texts_from_folders(src, names):\n",
    "    texts,labels = [],[]\n",
    "    MAX_DOCS = 10000\n",
    "    for idx,name in enumerate(names):\n",
    "        path = os.path.join(src, name)\n",
    "        i = 1\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            fpath = os.path.join(path, fname)\n",
    "            texts.append(open(fpath, encoding=\"utf8\").read())\n",
    "            labels.append(idx)\n",
    "            i = i + 1\n",
    "            if i > MAX_DOCS: \n",
    "                break\n",
    "    return texts,np.array(labels)\n",
    "\n",
    "#def tokenize(s): return re_tok.sub(r' \\1 ', s).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "train, train_y = texts_from_folders(path/'train',names)\n",
    "val, val_y = texts_from_folders(path/'test',names)\n",
    "\n",
    "#??load_files\n",
    "#files = load_files('C:/Users/xbabu/Babu/pers/PhD/MIST.7060 Data Analytics/Project/data/IMDB/train', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('punkt')\n",
    "\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemma_tokenizer(corpus): # a method to lemmatize corpus\n",
    "    corpus = ''.join([ch for ch in corpus if ch not in string.punctuation]) # remove punctuation\n",
    "    tokens = nltk.word_tokenize(corpus)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.remove('but')\n",
    "stopwords.remove('no')\n",
    "stopwords.remove('not')\n",
    "#print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "args = {\"stem\": False, \"lemmatize\": True}\n",
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤€‘’])')\n",
    "def re_tokenizer(s): return re_tok.sub(r' \\1 ', s).lower().split()\n",
    "#ctvec = CountVectorizer(ngram_range=(1,2), lowercase=True, tokenizer=re_tokenizer, stop_words=stopwords, min_df=4, max_df=0.8, strip_accents='unicode')\n",
    "#ctvec = CountVectorizer(ngram_range=(1,2), tokenizer=re_tokenizer, stop_words='english', min_df=4, max_df=0.8, strip_accents='unicode', lowercase=False)\n",
    "#ctvec = CountVectorizer(ngram_range=(1,3), tokenizer=lemma_tokenizer, stop_words='english', min_df=4, max_df=0.8, strip_accents='unicode', lowercase=False)\n",
    "ctvec = CountVectorizer(ngram_range=(1,1), tokenizer=lemma_tokenizer, stop_words=stopwords, min_df=4, max_df=0.8, strip_accents='unicode', lowercase=True)\n",
    "#ctvec = CountVectorizer(tokenizer=lambda text: tokenizer(text, **args), stop_words='english', strip_accents='ascii', min_df=0, max_df=1., vocabulary=None)\n",
    "#ctvec = CountVectorizer(tokenizer=tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_term_doc = ctvec.fit_transform(train)\n",
    "val_term_doc = ctvec.transform(val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<20000x27202 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1956250 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(trn_term_doc)\n",
    "trn_term_doc_dense = trn_term_doc.toarray()\n",
    "trn_term_doc\n",
    "#trn_term_doc_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = ctvec.get_feature_names()\n",
    "#dictionary\n",
    "#pd.DataFrame(trn_term_doc_dense, columns=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w0 = set([o.lower() for o in train[1].split(' ')])\n",
    "#w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, ..., 1, 1, 1, 1])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 20000\n",
      "Test set size: 20000\n"
     ]
    }
   ],
   "source": [
    "#print(len(w0))\n",
    "print(f'Training set size: {len(train)}')\n",
    "print(f'Test set size: {len(val)}')\n",
    "#print(ctvec.vocabulary_['writers'])\n",
    "#trn_term_doc[1, 934]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "We define the **log-count ratio** $r$ for each word $f$:\n",
    "\n",
    "$r = \\log \\frac{\\text{ratio of feature $f$ in positive documents}}{\\text ratio of feature $f$ in negative documents}}$\n",
    "\n",
    "where ratio of feature $f$ in positive documents is the number of times a positive has a feature divided by the number of positive documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with BOW NB matrix:\n",
      "Accuracy: 0.82695\n",
      "Confusion matrix:\n",
      "                  predicted positive  predicted negative\n",
      "actual positive                7654                2346\n",
      "actual negative                1115                8885\n",
      "precision = 0.8728475310753792 , recall = 0.7654 , F1-score = 0.8156001918056369\n",
      "Elapsed Time: 0.075 seconds.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "t0 = time()\n",
    "x = trn_term_doc\n",
    "y = train_y\n",
    "\n",
    "# grab the rows where the dependent varaible is 1\n",
    "p = x[y==1].sum(0)+1\n",
    "# grab the rows where the dependent varaible is 0\n",
    "q = x[y==0].sum(0)+1\n",
    "r = np.log((p/p.sum())/(q/q.sum()))\n",
    "b = np.log(len(p)/len(q))\n",
    "pre_preds = val_term_doc @r.T + b\n",
    "preds = pre_preds.T>0\n",
    "(preds == val_y).mean()\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('Results with BOW NB matrix:\\n' + 'Accuracy:', metrics.accuracy_score(val_y, preds.T))\n",
    "cm = pd.DataFrame(confusion_matrix(val_y, preds.T, labels=[1,0]),\n",
    "                  index=['actual positive', 'actual negative'],\n",
    "                  columns=['predicted positive', 'predicted negative'])\n",
    "print('Confusion matrix:\\n', cm)\n",
    "TP = cm.at['actual positive','predicted positive']\n",
    "FP = cm.at['actual negative','predicted positive']\n",
    "FN = cm.at['actual positive','predicted negative']\n",
    "print('precision =', TP/(TP+FP), ', recall =', TP/(TP+FN), ', F1-score =', 2*TP/(2*TP+FP+FN))\n",
    "\n",
    "print(\"Elapsed Time: %0.3f seconds.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarized Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with BOW NB.sign() matrix:\n",
      "Accuracy: 0.83485\n",
      "Confusion matrix:\n",
      "                  predicted positive  predicted negative\n",
      "actual positive                7810                2190\n",
      "actual negative                1113                8887\n",
      "precision = 0.8752661660876387 , recall = 0.781 , F1-score = 0.8254505099614227\n",
      "Elapsed Time: 0.063 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Turns out that the version where we are actually looking at\n",
    "# how often a word appears like \"absurd\" appeared twice. It turns out\n",
    "# at least for this problem and quite often it doesn't matter\n",
    "# whether \"absurd\" appeared twice or once\n",
    "# all that matters is it appeared so what we tend to d here is\n",
    "# take the term document matrix and do doc sign.\n",
    "# doc.sign replaces anything positive as 1 and \n",
    "# anything negative with -1 so it binarizes it\n",
    "# it gives a better result\n",
    "t0 = time()\n",
    "pre_preds = val_term_doc.sign() @r.T + b\n",
    "#preds\n",
    "preds = pre_preds.T>0\n",
    "(preds == val_y).mean()\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('Results with BOW NB.sign() matrix:\\n' + 'Accuracy:', metrics.accuracy_score(val_y, preds.T))\n",
    "cm = pd.DataFrame(confusion_matrix(val_y, preds.T, labels=[1,0]),\n",
    "                  index=['actual positive', 'actual negative'],\n",
    "                  columns=['predicted positive', 'predicted negative'])\n",
    "print('Confusion matrix:\\n', cm)\n",
    "TP = cm.at['actual positive','predicted positive']\n",
    "FP = cm.at['actual negative','predicted positive']\n",
    "FN = cm.at['actual positive','predicted negative']\n",
    "print('precision =', TP/(TP+FP), ', recall =', TP/(TP+FN), ', F1-score =', 2*TP/(2*TP+FP+FN))\n",
    "print(\"Elapsed Time: %0.3f seconds.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "instead of using the coefficients r.T, let us learn them using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with BOW LR :\n",
      "Accuracy: 0.87565\n",
      "Confusion matrix:\n",
      "                  predicted positive  predicted negative\n",
      "actual positive                8704                1296\n",
      "actual negative                1191                8809\n",
      "precision = 0.8796361798888327 , recall = 0.8704 , F1-score = 0.8749937170143252\n",
      "Elapsed Time: 1.010 seconds.\n"
     ]
    }
   ],
   "source": [
    "# In theory NB sounds ok, but rather than assuming that\n",
    "# we should use these coefficients r.T why don't we learn them\n",
    "# let su create a logistic regression\n",
    "m = LogisticRegression(C=0.1, dual=False)\n",
    "t0 = time()\n",
    "m.fit(x,y)\n",
    "preds = m.predict(val_term_doc)\n",
    "(preds==val_y).mean()\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('Results with BOW LR :\\n' + 'Accuracy:', metrics.accuracy_score(val_y, preds))\n",
    "cm = pd.DataFrame(confusion_matrix(val_y, preds, labels=[1,0]),\n",
    "                  index=['actual positive', 'actual negative'],\n",
    "                  columns=['predicted positive', 'predicted negative'])\n",
    "print('Confusion matrix:\\n', cm)\n",
    "TP = cm.at['actual positive','predicted positive']\n",
    "FP = cm.at['actual negative','predicted positive']\n",
    "FN = cm.at['actual positive','predicted negative']\n",
    "print('precision =', TP/(TP+FP), ', recall =', TP/(TP+FN), ', F1-score =', 2*TP/(2*TP+FP+FN))\n",
    "print(\"Elapsed Time: %0.3f seconds.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarized LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with BOW LR.sign() matrix:\n",
      "Accuracy: 0.87655\n",
      "Confusion matrix:\n",
      "                  predicted positive  predicted negative\n",
      "actual positive                8767                1233\n",
      "actual negative                1236                8764\n",
      "precision = 0.8764370688793361 , recall = 0.8767 , F1-score = 0.8765685147227916\n",
      "Elapsed Time: 0.853 seconds.\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "m = LogisticRegression(C=0.1, dual=False)\n",
    "m.fit(trn_term_doc.sign(),y)\n",
    "preds = m.predict(val_term_doc.sign())\n",
    "(preds==val_y).mean()\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('Results with BOW LR.sign() matrix:\\n' + 'Accuracy:', metrics.accuracy_score(val_y, preds))\n",
    "cm = pd.DataFrame(confusion_matrix(val_y, preds, labels=[1,0]),\n",
    "                  index=['actual positive', 'actual negative'],\n",
    "                  columns=['predicted positive', 'predicted negative'])\n",
    "print('Confusion matrix:\\n', cm)\n",
    "TP = cm.at['actual positive','predicted positive']\n",
    "FP = cm.at['actual negative','predicted positive']\n",
    "FN = cm.at['actual positive','predicted negative']\n",
    "print('precision =', TP/(TP+FP), ', recall =', TP/(TP+FN), ', F1-score =', 2*TP/(2*TP+FP+FN))\n",
    "print(\"Elapsed Time: %0.3f seconds.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with BOW SVM matrix:\n",
      "Accuracy: 0.83655\n",
      "Results with BOW SVM matrix:\n",
      "Accuracy: 0.83655\n",
      "Confusion matrix:\n",
      "                  predicted positive  predicted negative\n",
      "actual positive                8223                1777\n",
      "actual negative                1492                8508\n",
      "precision = 0.8464230571281524 , recall = 0.8223 , F1-score = 0.8341871671316257\n",
      "Elapsed Time: 631.957 seconds.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "t0 = time()\n",
    "svc = SVC(kernel='linear', C=1)\n",
    "svc.fit(x,y)\n",
    "#print(\"Elapsed Time: %0.3f seconds.\" % (time() - t0))\n",
    "#t0 = time()\n",
    "preds = svc.predict(val_term_doc)\n",
    "print('Results with BOW SVM matrix:\\n' + 'Accuracy:', metrics.accuracy_score(val_y, preds))\n",
    "#print(\"Elapsed Time: %0.3f seconds.\" % (time() - t0))\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('Results with BOW SVM matrix:\\n' + 'Accuracy:', metrics.accuracy_score(val_y, preds))\n",
    "cm = pd.DataFrame(confusion_matrix(val_y, preds, labels=[1,0]),\n",
    "                  index=['actual positive', 'actual negative'],\n",
    "                  columns=['predicted positive', 'predicted negative'])\n",
    "print('Confusion matrix:\\n', cm)\n",
    "TP = cm.at['actual positive','predicted positive']\n",
    "FP = cm.at['actual negative','predicted positive']\n",
    "FN = cm.at['actual positive','predicted negative']\n",
    "print('precision =', TP/(TP+FP), ', recall =', TP/(TP+FN), ', F1-score =', 2*TP/(2*TP+FP+FN))\n",
    "print(\"Elapsed Time: %0.3f seconds.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarized SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with BOW SVM.sign() matrix:\n",
      "Accuracy: 0.84345\n",
      "Results with BOW SVM.sign() matrix:\n",
      "Accuracy: 0.84345\n",
      "Confusion matrix:\n",
      "                  predicted positive  predicted negative\n",
      "actual positive                8329                1671\n",
      "actual negative                1460                8540\n",
      "precision = 0.8508529982633568 , recall = 0.8329 , F1-score = 0.8417807873060791\n",
      "Elapsed Time: 622.871 seconds.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "t0 = time()\n",
    "svc = SVC(kernel='linear', C=1)\n",
    "svc.fit(trn_term_doc.sign(),y)\n",
    "#print(\"Elapsed Time: %0.3f seconds.\" % (time() - t0))\n",
    "#t0 = time()\n",
    "preds = svc.predict(val_term_doc.sign())\n",
    "print('Results with BOW SVM.sign() matrix:\\n' + 'Accuracy:', metrics.accuracy_score(val_y, preds))\n",
    "#print(\"Elapsed Time: %0.3f seconds.\" % (time() - t0))\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('Results with BOW SVM.sign() matrix:\\n' + 'Accuracy:', metrics.accuracy_score(val_y, preds))\n",
    "cm = pd.DataFrame(confusion_matrix(val_y, preds, labels=[1,0]),\n",
    "                  index=['actual positive', 'actual negative'],\n",
    "                  columns=['predicted positive', 'predicted negative'])\n",
    "print('Confusion matrix:\\n', cm)\n",
    "TP = cm.at['actual positive','predicted positive']\n",
    "FP = cm.at['actual negative','predicted positive']\n",
    "FN = cm.at['actual positive','predicted negative']\n",
    "print('precision =', TP/(TP+FP), ', recall =', TP/(TP+FN), ', F1-score =', 2*TP/(2*TP+FP+FN))\n",
    "print(\"Elapsed Time: %0.3f seconds.\" % (time() - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = trn_term_doc.sign()\n",
    "y = train_y\n",
    "# x[y==1].sum(0) is to calculate the ratio of each feature in positive and no negative by dividing the toekn counts.\n",
    "\n",
    "val_x = val_term_doc.sign()\n",
    "p = x[y==1].sum(0)+1\n",
    "q = x[y==0].sum(0)+1\n",
    "r = np.log((p/p.sum())/(q/q.sum()))\n",
    "b = np.log(len(p)/len(q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NB-LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with BOW NB-LR matrix:\n",
      "Accuracy: 0.88525\n",
      "Confusion matrix:\n",
      "                  predicted positive  predicted negative\n",
      "actual positive                8867                1133\n",
      "actual negative                1162                8838\n",
      "precision = 0.884136005583807 , recall = 0.8867 , F1-score = 0.8854161465874482\n",
      "Elapsed Time: 0.677 seconds.\n"
     ]
    }
   ],
   "source": [
    "################ NB-LR #############################\n",
    "# take the term-document matrix and multiply by r\n",
    "# every where 0 appears, 0 appears in x_nb and \n",
    "# every where 1 appears in TD, the r appears there\n",
    "t0 = time()\n",
    "x_nb = x.multiply(r)\n",
    "# we are going to use x_nb as independent variables instead in the\n",
    "# logistic regression\n",
    "m = LogisticRegression(dual=False, C=0.1)\n",
    "m.fit(x_nb, y)\n",
    "\n",
    "# do the samething for the validation set and get the predictions\n",
    "# to get a better result\n",
    "val_x_nb = val_x.multiply(r)\n",
    "preds = m.predict(val_x_nb)\n",
    "#(preds.T==val_y).mean()\n",
    "print('Results with BOW NB-LR matrix:\\n' + 'Accuracy:', metrics.accuracy_score(val_y, preds.T))\n",
    "cm = pd.DataFrame(confusion_matrix(val_y, preds.T, labels=[1,0]),\n",
    "                  index=['actual positive', 'actual negative'],\n",
    "                  columns=['predicted positive', 'predicted negative'])\n",
    "print('Confusion matrix:\\n', cm)\n",
    "TP = cm.at['actual positive','predicted positive']\n",
    "FP = cm.at['actual negative','predicted positive']\n",
    "FN = cm.at['actual positive','predicted negative']\n",
    "print('precision =', TP/(TP+FP), ', recall =', TP/(TP+FN), ', F1-score =', 2*TP/(2*TP+FP+FN))\n",
    "print(\"Elapsed Time: %0.3f seconds.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarized NB-LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with BOW NB-LR matrix:\n",
      "Accuracy: 0.88525\n",
      "Confusion matrix:\n",
      "                  predicted positive  predicted negative\n",
      "actual positive                8867                1133\n",
      "actual negative                1162                8838\n",
      "precision = 0.884136005583807 , recall = 0.8867 , F1-score = 0.8854161465874482\n",
      "Elapsed Time: 0.720 seconds.\n"
     ]
    }
   ],
   "source": [
    "################ NB-LR Sign() #############################\n",
    "# take the term-document matrix and multiply by r\n",
    "# every where 0 appears, 0 appears in x_nb and \n",
    "# every where 1 appears in TD, the r appears there\n",
    "t0 = time()\n",
    "x_nb = x.sign().multiply(r)\n",
    "# we are going to use x_nb as independent variables instead in the\n",
    "# logistic regression\n",
    "m = LogisticRegression(dual=False, C=0.1)\n",
    "m.fit(x_nb, y)\n",
    "\n",
    "# do the samething for the validation set and get the predictions\n",
    "# to get a better result\n",
    "val_x_nb = val_x.sign().multiply(r)\n",
    "preds = m.predict(val_x_nb)\n",
    "#(preds.T==val_y).mean()\n",
    "print('Results with BOW NB-LR matrix:\\n' + 'Accuracy:', metrics.accuracy_score(val_y, preds.T))\n",
    "cm = pd.DataFrame(confusion_matrix(val_y, preds.T, labels=[1,0]),\n",
    "                  index=['actual positive', 'actual negative'],\n",
    "                  columns=['predicted positive', 'predicted negative'])\n",
    "print('Confusion matrix:\\n', cm)\n",
    "TP = cm.at['actual positive','predicted positive']\n",
    "FP = cm.at['actual negative','predicted positive']\n",
    "FN = cm.at['actual positive','predicted negative']\n",
    "print('precision =', TP/(TP+FP), ', recall =', TP/(TP+FN), ', F1-score =', 2*TP/(2*TP+FP+FN))\n",
    "print(\"Elapsed Time: %0.3f seconds.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NB-SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 394.508 seconds.\n",
      "Results with BOW NB-SVM matrix:\n",
      "Accuracy: 0.83785\n",
      "Confusion matrix:\n",
      "                  predicted positive  predicted negative\n",
      "actual positive                8172                1828\n",
      "actual negative                1415                8585\n",
      "precision = 0.8524042974861792 , recall = 0.8172 , F1-score = 0.8344310001531628\n",
      "Elapsed Time: 73.786 seconds.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "t0 = time()\n",
    "svc = SVC(kernel='linear', C=1e5)\n",
    "svc.fit(x_nb, y)\n",
    "print(\"Elapsed Time: %0.3f seconds.\" % (time() - t0))\n",
    "\n",
    "t0 = time()\n",
    "preds = svc.predict(val_x_nb)\n",
    "print('Results with BOW NB-SVM matrix:\\n' + 'Accuracy:', metrics.accuracy_score(val_y, preds.T))\n",
    "cm = pd.DataFrame(confusion_matrix(val_y, preds, labels=[1,0]),\n",
    "                  index=['actual positive', 'actual negative'],\n",
    "                  columns=['predicted positive', 'predicted negative'])\n",
    "print('Confusion matrix:\\n', cm)\n",
    "TP = cm.at['actual positive','predicted positive']\n",
    "FP = cm.at['actual negative','predicted positive']\n",
    "FN = cm.at['actual positive','predicted negative']\n",
    "print('precision =', TP/(TP+FP), ', recall =', TP/(TP+FN), ', F1-score =', 2*TP/(2*TP+FP+FN))\n",
    "print(\"Elapsed Time: %0.3f seconds.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
